{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615ea033",
   "metadata": {},
   "source": [
    "# LLM Question Answering Tool\n",
    "\n",
    "This notebook demonstrates how to build a tool that takes a technical question and generates an explanation using either an OpenAI GPT model or a locally hosted LLaMA model via Ollama. The project illustrates prompt processing, model integration, and streaming output to the notebook interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b3f87",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b70a68",
   "metadata": {},
   "source": [
    "### Define Model Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models as constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771416d",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac47d21",
   "metadata": {},
   "source": [
    "### Function to Stream Model Responses\n",
    "This function detects whether to use an OpenAI GPT model or an Ollama LLaMA model and streams the model output directly to the notebook as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df301b0-5c00-4b82-bc7f-3293e0ce112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(model, question):\n",
    "    if \"gpt\" in model.lower():\n",
    "        # Use OpenAI client for GPT models\n",
    "        openai = OpenAI()\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": question}],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    else:\n",
    "        # For Ollama models, use requests directly       \n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        \n",
    "        stream = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": model, \n",
    "                \"prompt\": question, \n",
    "                \"stream\": True},\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for line in stream.iter_lines():\n",
    "            if line:\n",
    "                json_line = json.loads(line.decode('utf-8'))\n",
    "                if \"response\" in json_line:\n",
    "                    token = json_line[\"response\"]\n",
    "                    response += token\n",
    "                    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7073cae8-74b5-4356-ad35-6c2fe6128188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The code snippet you've provided utilizes Python's `yield from` statement in conjunction with a set comprehension. Let's break it down step-by-step to understand what it does and why.\n",
       "\n",
       "### Breakdown of the Code\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   \n",
       "   - This part creates a set of authors from a collection of `books`. \n",
       "   - It iterates over each `book` in the `books` iterable.\n",
       "   - It uses `book.get(\"author\")` to retrieve the \"author\" key's value from each book.\n",
       "   - The condition `if book.get(\"author\")` ensures that only books with a valid (non-None, non-empty) author are included in the resulting set.\n",
       "   - The result is a set containing unique author names, as sets automatically handle duplicates.\n",
       "\n",
       "2. **Yield from**: \n",
       "   python\n",
       "   yield from ...\n",
       "   \n",
       "   - The `yield from` expression is used within a generator function. It allows one generator to yield values from another iterable (in this case, the set of authors).\n",
       "   - This means that when the generator containing this code is called, it will yield each author in the set one at a time.\n",
       "\n",
       "### Why This Code is Used\n",
       "\n",
       "- **Production of Unique Results**: The use of a set ensures that the authors yielded are unique; if multiple books have the same author, that author will only be yielded once.\n",
       "- **Efficiency**: Using `yield from` allows for lazily generating values one at a time as they are needed, rather than generating all values at once. This can be more memory-efficient, especially when dealing with large lists of books.\n",
       "- **Filtering**: The `if book.get(\"author\")` condition serves to filter out any books that do not have an author listed, thus only valid author names are processed and yielded.\n",
       "\n",
       "### Example Usage\n",
       "\n",
       "If you had a list of dictionaries (`books`) like this:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book One\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book Two\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book Three\", \"author\": None},\n",
       "    {\"title\": \"Book Four\", \"author\": \"Author A\"},\n",
       "]\n",
       "\n",
       "\n",
       "The code would produce the following unique authors:\n",
       "- \"Author A\"\n",
       "- \"Author B\"\n",
       "\n",
       "In a generator function, this could be utilized to provide a stream of authors dynamically, which is useful in various applications such as rendering authors in a user interface, processing or filtering data, or simply iterating through them in a loop."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "# Here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "stream_response(MODEL_GPT, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This line of code uses a combination of Python's yield from statement, dictionary get method, list comprehension, and generator expressions.\n",
       "\n",
       "Here's a breakdown of what it does:\n",
       "\n",
       "- `books` is assumed to be an iterable (e.g., a list or dictionary) containing objects with certain attributes.\n",
       "- The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` generates a new dictionary where each key-value pair corresponds to the author of a book. The key is the string \"author\" and the value is the actual author name.\n",
       "\n",
       "Here's why it does that:\n",
       "\n",
       "- `for book in books` iterates over each item (`book`) in the list.\n",
       "- `if book.get(\"author\")` filters out items where the key `\"author\"` doesn't exist. In Python, dictionary keys are case-sensitive (e.g., \"Author\" would not be matched by `.get(\"author\")`). However, if you want to make it case-insensitive, consider using `.get(\"author\", None)` instead.\n",
       "- `book.get(\"author\")` returns the author name of a book. This could raise an exception if the key doesn't exist.\n",
       "\n",
       "Now, here's where `yield from` comes in:\n",
       "\n",
       "- `yield from { ... }` takes a sequence (like the dictionary comprehension above) and \"unpacks\" it as if each item was individually yielded by the function itself.\n",
       "- The outer function then yields each value one by one, producing an iterator over all values in the dictionary.\n",
       "\n",
       "So, putting it all together, this line of code generates a generator that produces an author's name for each book where the key `\"author\"` exists. This allows you to iterate over these authors without having to load all books into memory at once.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"id\": 1, \"title\": \"Book A\", \"author\": \"John Smith\"},\n",
       "    {\"id\": 2, \"title\": \"Book B\", \"author\": \"Jane Doe\"}\n",
       "]\n",
       "\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "Output:\n",
       "\n",
       "```python\n",
       "John Smith\n",
       "Jane Doe\n",
       "```\n",
       "\n",
       "This code snippet is equivalent to the following, but without using dictionary comprehension and `yield from`:\n",
       "\n",
       "```python\n",
       "def get_authors(books):\n",
       "    authors = []\n",
       "    for book in books:\n",
       "        if \"author\" in book:\n",
       "            author = book[\"author\"]\n",
       "            # You could also raise an exception here instead of silently dropping books\n",
       "            authors.append(author)\n",
       "    return authors\n",
       "\n",
       "# Usage\n",
       "books = [\n",
       "    {\"id\": 1, \"title\": \"Book A\", \"author\": \"John Smith\"},\n",
       "    {\"id\": 2, \"title\": \"Book B\", \"author\": \"Jane Doe\"}\n",
       "]\n",
       "\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "stream_response(MODEL_LLAMA, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b627f4a-5815-4ea4-a39d-46bfab0f9290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
