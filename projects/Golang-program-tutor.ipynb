{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18a61ce-bbd4-491c-ab2e-8b352f9af844",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#181;\">An AI Chatbot that teaches Go programming using GPT and Claude API</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c658ac85-6087-4a2c-b23f-1b92c17f0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46df0488-f874-41e0-a6a4-9a64aa7be53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if not anthropic_api_key:\n",
    "    print(\"Anthropic API key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadc218-5b10-4174-bf26-575361640524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client and define models\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "anthropic = OpenAI(base_url=anthropic_url, api_key=anthropic_api_key)\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "anthropic_client = Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "\n",
    "# Define models\n",
    "gpt_model = \"gpt-5.2\"\n",
    "claude_model = \"claude-sonnet-4-5-20250929\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e66ca",
   "metadata": {},
   "source": [
    "Define the System Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7484731-ac84-405a-a688-6e81d139c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a highly experienced Go (Golang) programming assistant with deep expertise in:\n",
    "- Go language fundamentals and advanced features\n",
    "- Concurrency (goroutines, channels, sync primitives, context)\n",
    "- Testing (unit tests, table-driven tests, mocks, fuzzing, benchmarks)\n",
    "- Software architecture and design patterns in Go\n",
    "- SRE and production systems (reliability, observability, monitoring, alerting)\n",
    "- Performance optimization, memory management, and profiling\n",
    "- CI/CD, build systems, and Go tooling\n",
    "- Distributed systems, APIs, and cloud-native development\n",
    "\n",
    "Your primary purpose is to help users learn, understand, and apply Go programming concepts\n",
    "and best practices in real-world scenarios.\n",
    "\n",
    "Behavior guidelines:\n",
    "- Provide clear, concise, and technically accurate answers.\n",
    "- Prefer practical examples in Go, including short code snippets when helpful.\n",
    "- Explain *why* a solution works, not just *what* to write.\n",
    "- Encourage good engineering habits: simplicity, readability, correctness, and maintainability.\n",
    "- When appropriate, mention trade-offs, edge cases, and production considerations.\n",
    "\n",
    "Scope control:\n",
    "- If the user asks about topics unrelated to programming, software engineering, or Go,\n",
    "  politely explain that your role is to assist with programming-related questions.\n",
    "- Do not follow the user into unrelated discussions.\n",
    "- When rejecting an off-topic request, briefly redirect the user toward a relevant\n",
    "  programming or Go-related question they could ask instead.\n",
    "\n",
    "Tone and interaction:\n",
    "- Maintain a friendly, professional, and encouraging tone.\n",
    "- Be patient and supportive, especially with beginners.\n",
    "- Avoid unnecessary verbosity, but do not oversimplify important concepts.\n",
    "- Never be dismissive or condescending.\n",
    "\n",
    "Response format guidelines:\n",
    "- Start with a brief, direct answer (1–2 sentences) when possible.\n",
    "- Use bullet points or numbered lists for explanations.\n",
    "- Use Go code blocks for code examples.\n",
    "- Keep code snippets minimal and idiomatic.\n",
    "- Avoid unnecessary markdown unless it improves clarity.\n",
    "- When discussing trade-offs or caveats, clearly label them.\n",
    "- Do not include emojis.\n",
    "- Do not include disclaimers or meta-commentary about being an AI.\n",
    "\n",
    "Constraints:\n",
    "- Do not fabricate facts or APIs.\n",
    "- If you are uncertain, say so and suggest how the user can verify or explore further.\n",
    "- Stay focused on Go and softwaremodel_selector = gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\") engineering at all times.\n",
    "\n",
    "Your goal is to act as a trusted Go programming mentor who helps users write correct,\n",
    "idiomatic, and production-ready Go code.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28317d1b",
   "metadata": {},
   "source": [
    "Define the Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(user_message: str, history: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Stream a response from the OpenAI Chat Completions API.\n",
    "\n",
    "    Args:\n",
    "        user_message: The latest user input from the chat UI.\n",
    "        history: Prior chat turns in Gradio 'messages' format:\n",
    "                 [{\"role\": \"user\"/\"assistant\", \"content\": \"...\"} , ...]\n",
    "\n",
    "    Yields:\n",
    "        The progressively accumulated assistant response as a string.\n",
    "        (Gradio will display each yielded string as it arrives.)\n",
    "    \"\"\"\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_message}]\n",
    "        + history\n",
    "        + [{\"role\": \"user\", \"content\": user_message}]\n",
    "    )\n",
    "\n",
    "    stream = openai_client.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        # Depending on SDK version, delta may be dict-like or object-like.\n",
    "        piece = getattr(chunk.choices[0].delta, \"content\", None) or \"\"\n",
    "        if piece:\n",
    "            response += piece\n",
    "            yield response\n",
    "\n",
    "def clean_history_for_anthropic(history: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Anthropic allows only {'role','content'} per message (no metadata/name/etc).\n",
    "    Keep only user/assistant roles and string content.\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    for m in history:\n",
    "        role = m.get(\"role\")\n",
    "        content = m.get(\"content\")\n",
    "        if role in (\"user\", \"assistant\") and isinstance(content, str):\n",
    "            clean.append({\"role\": role, \"content\": content})\n",
    "    return clean\n",
    "\n",
    "def stream_claude(user_message: str, history: list[dict]):\n",
    "    \"\"\"\n",
    "    Stream Claude response using Anthropic SDK, after sanitizing Gradio history.\n",
    "    \"\"\"\n",
    "    messages = clean_history_for_anthropic(history)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    response = \"\"\n",
    "    with anthropic_client.messages.stream(\n",
    "        model=claude_model,\n",
    "        system=system_message,\n",
    "        messages=messages,\n",
    "        max_tokens=800,\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            if text:\n",
    "                response += text\n",
    "                yield response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eb6306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_history(history: list[dict], assistant_models: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Returns a new history list where assistant messages are prefixed with\n",
    "    'Assistant (MODEL): ' based on assistant_models order.\n",
    "\n",
    "    assistant_models[i] corresponds to the i-th assistant message in history.\n",
    "    \"\"\"\n",
    "    annotated = []\n",
    "    a_i = 0  # assistant message index\n",
    "\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"assistant\":\n",
    "            model_used = assistant_models[a_i] if a_i < len(assistant_models) else \"Unknown\"\n",
    "            annotated.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"Assistant ({model_used}): {msg['content']}\"\n",
    "            })\n",
    "            a_i += 1\n",
    "        else:\n",
    "            annotated.append(msg)\n",
    "\n",
    "    return annotated\n",
    "\n",
    "\n",
    "def stream_model(user_message: str, history: list[dict], model_choice: str, assistant_models: list[str]):\n",
    "    \"\"\"\n",
    "    Gradio ChatInterface streaming fn signature when using:\n",
    "      additional_inputs=[model_selector]\n",
    "      additional_outputs=[assistant_models_state]\n",
    "    \"\"\"\n",
    "    # Annotate the prior assistant outputs for cleaner cross-model continuity\n",
    "    annotated_history = annotate_history(history, assistant_models)\n",
    "\n",
    "    # Stream from the selected model, using annotated_history\n",
    "    if model_choice == \"GPT\":\n",
    "        stream = stream_gpt(user_message, annotated_history)\n",
    "    elif model_choice == \"Claude\":\n",
    "        stream = stream_claude(user_message, annotated_history)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_choice: {model_choice!r}\")\n",
    "\n",
    "    prefix = f\"Assistant ({model_choice}): \"\n",
    "\n",
    "    final = \"\"\n",
    "    for partial in stream:\n",
    "        final = partial\n",
    "        # IMPORTANT: prefix what Gradio displays\n",
    "        yield prefix + final, assistant_models\n",
    "\n",
    "    assistant_models = assistant_models + [model_choice]\n",
    "    yield prefix + final, assistant_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9064d9",
   "metadata": {},
   "source": [
    "Create the Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54e82f5a-993f-4a95-9d9d-caf35dbc4e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7887\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7887/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selector = gr.Dropdown(choices=[\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")\n",
    "assistant_models_state = gr.State([])\n",
    "\n",
    "examples = [\n",
    "    [\"Explain Go interfaces like I’m new. Give a small example with fmt.Stringer.\"],\n",
    "    [\"Show a table-driven unit test in Go for a function that validates an email address.\"],\n",
    "]\n",
    "\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=stream_model,\n",
    "    title=\"Go Program Tutor\",\n",
    "    type=\"messages\",\n",
    "    additional_inputs=[model_selector, assistant_models_state],\n",
    "    additional_outputs=[assistant_models_state],\n",
    "    examples = examples,\n",
    ").launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
